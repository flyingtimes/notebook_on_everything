import os
import json
import glob
import argparse
from datetime import datetime
from typing import List, Dict, Tuple
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models import GPTModel
from dotenv import load_dotenv
load_dotenv()
from deepeval.models.base_model import DeepEvalBaseLLM
from openai import OpenAI
from jinja2 import Template

class OpenRouterModel(DeepEvalBaseLLM):
    def __init__(self, model: str, api_key: str):
        self.api_key = api_key
        super().__init__(model)
        # ç¡®ä¿ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹åç§°ï¼Œè¦†ç›–çˆ¶ç±»å¯èƒ½çš„ä¿®æ”¹
        self.model_name = model
      
    def load_model(self):
        return OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.api_key
        )
      
    def generate(self, prompt: str) -> str:
        client = self.load_model()
        response = client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
      
    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)
      
    def get_model_name(self):
        return self.model_name

def create_openrouter_model(model_name: str = "openai/gpt-4", api_key: str = None):
    """åˆ›å»ºOpenRouteræ¨¡å‹å®ä¾‹"""
    if not api_key:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡æˆ–æä¾›api_keyå‚æ•°")
      
    return OpenRouterModel(
        model=model_name,
        api_key=api_key
    )

def get_model_list() -> List[str]:
    """ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åˆ—è¡¨"""
    model_list_str = os.getenv("MODEL_LIST", "openai/gpt-4")
    return [model.strip() for model in model_list_str.split(",") if model.strip()]

def read_prompt_template(file_path: str = "prompts/1.prompts", subject: str = None) -> str:
    """è¯»å–æç¤ºè¯æ¨¡æ¿æ–‡ä»¶å¹¶å¡«å……å˜é‡"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            template_content = f.read().strip()
            
        # ä½¿ç”¨Jinja2æ¨¡æ¿å¼•æ“å¤„ç†æ¨¡æ¿
        template = Template(template_content)
        
        # å¦‚æœæä¾›äº†subjectå‚æ•°ï¼Œåˆ™è¿›è¡Œå˜é‡æ›¿æ¢
        if subject:
            content = template.render(subject=subject)
        else:
            content = template_content
            
        return content
    except FileNotFoundError:
        print(f"æç¤ºè¯æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return None
    except Exception as e:
        print(f"è¯»å–æç¤ºè¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def sanitize_filename(model_name: str) -> str:
    """æ¸…ç†æ¨¡å‹åç§°ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶å"""
    # æ›¿æ¢ä¸é€‚åˆæ–‡ä»¶åçš„å­—ç¬¦
    return model_name.replace("/", "_").replace(":", "_").replace("?", "_").replace("*", "_")

def generate_content_with_models(prompt_template: str, model_list: List[str], api_key: str):
    """ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç”Ÿæˆå†…å®¹å¹¶ä¿å­˜åˆ°inputæ–‡ä»¶å¤¹"""
    # ç¡®ä¿inputæ–‡ä»¶å¤¹å­˜åœ¨
    input_dir = "./input"
    if not os.path.exists(input_dir):
        os.makedirs(input_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {input_dir}")
    
    results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for i, model_name in enumerate(model_list, 1):
        print(f"\n[{i}/{len(model_list)}] æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = create_openrouter_model(model_name, api_key)
            
            # ç”Ÿæˆå†…å®¹
            print("æ­£åœ¨ç”Ÿæˆå†…å®¹...")
            generated_content = model.generate(prompt_template)
            # å»æ‰ç©ºè¡Œ
            generated_content = "\n".join(line for line in generated_content.split("\n") if line.strip())
            safe_model_name = sanitize_filename(model_name)
            filename = f"{safe_model_name}_{timestamp}.txt"
            file_path = os.path.join(input_dir, filename)
            
            # ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(generated_content)
            
            print(f"âœ… å†…å®¹å·²ä¿å­˜åˆ°: {file_path}")
            
            # è®°å½•ç»“æœ
            results.append({
                "model": model_name,
                "filename": filename,
                "file_path": file_path,
                "content_length": len(generated_content),
                "status": "success",
                "content": generated_content  # æ·»åŠ å†…å®¹ç”¨äºåç»­è¯„ä¼°
            })
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} ç”Ÿæˆå¤±è´¥: {e}")
            results.append({
                "model": model_name,
                "filename": None,
                "file_path": None,
                "content_length": 0,
                "status": "failed",
                "error": str(e),
                "content": None
            })
    
    return results

def read_text_files(directory: str = "./input") -> Dict[str, str]:
    """è¯»å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
    text_files = {}
      
    # æ”¯æŒå¤šç§æ–‡æœ¬æ–‡ä»¶æ ¼å¼
    patterns = ["*.txt", "*.md", "*.text"]
      
    for pattern in patterns:
        file_pattern = os.path.join(directory, pattern)
        for file_path in glob.glob(file_pattern):
            filename = os.path.basename(file_path)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    # æå–å®é™…ç”Ÿæˆçš„å†…å®¹ï¼ˆè·³è¿‡æ–‡ä»¶å¤´ä¿¡æ¯ï¼‰
                    if "=" * 50 in content:
                        content = content.split("=" * 50, 1)[1].strip()
                    if content:  # åªä¿å­˜éç©ºæ–‡ä»¶
                        text_files[filename] = content
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
      
    return text_files

def generate_evaluation_standard(sample_text: str, user_requirement: str, model: OpenRouterModel) -> List[str]:
    """ä½¿ç”¨G-Evalç”Ÿæˆè¯„ä¼°æ ‡å‡†"""
    print("æ­£åœ¨ç”Ÿæˆè¯„ä¼°æ ‡å‡†...")
      
    # åˆ›å»ºG-EvalæŒ‡æ ‡æ¥ç”Ÿæˆè¯„ä¼°æ­¥éª¤
    criteria = f"""
    æ ¹æ®ç”¨æˆ·éœ€æ±‚è¯„ä¼°AIç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}
      
    """
      
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æµ‹è¯•ç”¨ä¾‹æ¥ç”Ÿæˆè¯„ä¼°æ­¥éª¤
    temp_test_case = LLMTestCase(
        input=user_requirement,
        actual_output=sample_text
    )
      
    temp_metric = GEval(
        name="æ–‡æœ¬è´¨é‡è¯„ä¼°",
        criteria=criteria,
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        model=model,  # ä½¿ç”¨OpenRouteræ¨¡å‹
        verbose_mode=False
    )
      
    # è¿è¡Œä¸€æ¬¡ä»¥ç”Ÿæˆè¯„ä¼°æ­¥éª¤
    temp_metric.measure(temp_test_case)
      
    return temp_metric.evaluation_steps

def evaluate_texts(text_files: Dict[str, str], user_requirement: str,   
                  evaluation_steps: List[str], model: OpenRouterModel) -> List[Tuple[str, float, str]]:
    """ä½¿ç”¨å›ºå®šçš„è¯„ä¼°æ­¥éª¤è¯„ä¼°æ‰€æœ‰æ–‡æœ¬"""
    print("å¼€å§‹è¯„ä¼°æ–‡æœ¬...")
      
    # åˆ›å»ºå›ºå®šè¯„ä¼°æ­¥éª¤çš„G-EvalæŒ‡æ ‡
    quality_metric = GEval(
        name="æ–‡æœ¬è´¨é‡è¯„ä¼°",
        evaluation_steps=evaluation_steps,
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        model=model,  # ä½¿ç”¨OpenRouteræ¨¡å‹
        threshold=0.7,
        verbose_mode=True 
    )
      
    results = []
      
    for filename, content in text_files.items():
        # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
        test_case = LLMTestCase(
            input=user_requirement,
            actual_output=content
        )
          
        # è¯„ä¼°
        score = quality_metric.measure(test_case)
        reason = quality_metric.reason
          
        results.append((filename, score, reason))
        print(f"è¯„ä¼°æ–‡ä»¶: {filename}, åˆ†æ•°: {score:.3f}")
      
    return results

def save_evaluation_steps(steps: List[str], file_path: str = "output/evaluation_steps.json"):
    """ä¿å­˜è¯„ä¼°æ­¥éª¤åˆ°JSONæ–‡ä»¶"""
    # ç¡®ä¿outputæ–‡ä»¶å¤¹å­˜åœ¨
    output_dir = os.path.dirname(file_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    data = {
        "evaluation_steps": steps,
        "created_at": datetime.now().isoformat(),
        "version": "1.0"
    }
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"è¯„ä¼°æ­¥éª¤å·²ä¿å­˜åˆ°: {file_path}")

def load_evaluation_steps(file_path: str = "output/evaluation_steps.json") -> List[str]:
    """ä»JSONæ–‡ä»¶åŠ è½½è¯„ä¼°æ­¥éª¤"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data["evaluation_steps"]
    except FileNotFoundError:
        print(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return None

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="AIå¤šæ¨¡å‹å†…å®¹ç”Ÿæˆä¸è¯„ä¼°ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py                    # ç›´æ¥è¯„ä¼°inputæ–‡ä»¶å¤¹ä¸­çš„ç°æœ‰æ–‡ä»¶
  python main.py --regen            # é‡æ–°ç”Ÿæˆå†…å®¹å¹¶è¯„ä¼°
  python main.py --regen --subject "æ–°çš„ä¸»é¢˜"  # ä½¿ç”¨æ–°ä¸»é¢˜é‡æ–°ç”Ÿæˆå†…å®¹
        """
    )
    
    parser.add_argument(
        "--regen", 
        action="store_true", 
        help="é‡æ–°ç”Ÿæˆå†…å®¹ã€‚å¦‚æœä¸ä½¿ç”¨æ­¤å‚æ•°ï¼Œç¨‹åºå°†ç›´æ¥è¯„ä¼°inputæ–‡ä»¶å¤¹ä¸­çš„ç°æœ‰æ–‡ä»¶"
    )
    
    parser.add_argument(
        "--subject", 
        type=str, 
        default="å¦‚ä½•æå‡äºŒå¹´çº§å°å­¦ç”Ÿçš„å†…é©±åŠ›",
        help="ç”Ÿæˆå†…å®¹çš„ä¸»é¢˜ï¼ˆé»˜è®¤ï¼šå¦‚ä½•æå‡äºŒå¹´çº§å°å­¦ç”Ÿçš„å†…é©±åŠ›ï¼‰"
    )
    
    parser.add_argument(
        "--eval-model", 
        type=str, 
        help="æŒ‡å®šè¯„ä¼°æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡EVAL_MODELæˆ–ç¬¬ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼‰"
    )
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    print("=== AIå¤šæ¨¡å‹å†…å®¹ç”Ÿæˆä¸è¯„ä¼°ç³»ç»Ÿ ===\n")
    
    if args.regen:
        print("ğŸ”„ æ¨¡å¼ï¼šé‡æ–°ç”Ÿæˆå†…å®¹å¹¶è¯„ä¼°")
    else:
        print("ğŸ“ æ¨¡å¼ï¼šç›´æ¥è¯„ä¼°ç°æœ‰æ–‡ä»¶")
    
    print(f"ğŸ“ ä¸»é¢˜ï¼š{args.subject}\n")
    
    # å®šä¹‰ç”¨æˆ·éœ€æ±‚ï¼ˆåŸºäºä¸»é¢˜ï¼‰
    user_requirement = f"{args.subject}"
    
    # 1. æ£€æŸ¥APIå¯†é’¥ï¼ˆä»…åœ¨éœ€è¦ç”Ÿæˆå†…å®¹æ—¶æ£€æŸ¥ï¼‰
    api_key = None
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # 2. è·å–æ¨¡å‹åˆ—è¡¨ï¼ˆä»…åœ¨éœ€è¦ç”Ÿæˆå†…å®¹æ—¶è·å–ï¼‰
    model_list = []
    generation_results = []
    
    if args.regen:
        model_list = get_model_list()
        print(f"ä»ç¯å¢ƒå˜é‡è¯»å–åˆ° {len(model_list)} ä¸ªæ¨¡å‹:")
        for i, model in enumerate(model_list, 1):
            print(f"  {i}. {model}")
        print()
        
        # 3. è¯»å–æç¤ºè¯æ¨¡æ¿
        prompt_template = read_prompt_template("prompts/1.prompts", subject=args.subject)
        if not prompt_template:
            print("âŒ æ— æ³•è¯»å–æç¤ºè¯æ¨¡æ¿ï¼Œç¨‹åºé€€å‡º")
            return
        
        print(f"âœ… æˆåŠŸè¯»å–æç¤ºè¯æ¨¡æ¿ (é•¿åº¦: {len(prompt_template)} å­—ç¬¦)")
        print(f"æç¤ºè¯é¢„è§ˆ: {prompt_template[:100]}...\n")
        
        # 4. ç”Ÿæˆå†…å®¹
        print("å¼€å§‹ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç”Ÿæˆå†…å®¹...")
        generation_results = generate_content_with_models(prompt_template, model_list, api_key)
        
        # 5. æ˜¾ç¤ºç”Ÿæˆç»“æœæ‘˜è¦
        print("\n=== ç”Ÿæˆç»“æœæ‘˜è¦ ===")
        successful_count = sum(1 for r in generation_results if r["status"] == "success")
        failed_count = len(generation_results) - successful_count
        
        print(f"æ€»æ¨¡å‹æ•°: {len(generation_results)}")
        print(f"æˆåŠŸç”Ÿæˆ: {successful_count}")
        print(f"ç”Ÿæˆå¤±è´¥: {failed_count}")
        
        if successful_count == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•å†…å®¹ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return
    else:
        print("â­ï¸  è·³è¿‡å†…å®¹ç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶")
    
    # 6. å¼€å§‹è¯„ä¼°é˜¶æ®µ
    print("\n" + "=" * 50)
    print("å¼€å§‹è¯„ä¼°é˜¶æ®µ...")
    print("=" * 50)
    
    # æ£€æŸ¥inputæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨æ–‡ä»¶
    if not os.path.exists("./input"):
        print("âŒ inputæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        if not args.regen:
            print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --regen å‚æ•°å¯ä»¥é‡æ–°ç”Ÿæˆå†…å®¹")
        return
    
    # 7. è¯»å–ç”Ÿæˆçš„æ–‡æœ¬æ–‡ä»¶
    text_files = read_text_files("./input")
    if not text_files:
        print("âŒ åœ¨ ./input ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶")
        if not args.regen:
            print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --regen å‚æ•°å¯ä»¥é‡æ–°ç”Ÿæˆå†…å®¹")
        return
    
    print(f"\næ‰¾åˆ° {len(text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶è¿›è¡Œè¯„ä¼°:")
    for filename in text_files.keys():
        print(f"  - {filename}")
    print()
    
    # é…ç½®è¯„ä¼°æ¨¡å‹
    eval_model_name = args.eval_model or os.getenv("OPENROUTER_MODEL")
    if not eval_model_name:
        if model_list:
            eval_model_name = model_list[0]
        else:
            eval_model_name = "openai/gpt-4"  # é»˜è®¤è¯„ä¼°æ¨¡å‹
    
    print(f"ä½¿ç”¨è¯„ä¼°æ¨¡å‹: {eval_model_name}")
    
    # æ£€æŸ¥è¯„ä¼°æ¨¡å‹çš„APIå¯†é’¥
    if not api_key:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            print("âŒ é”™è¯¯: è¯„ä¼°é˜¶æ®µéœ€è¦OPENROUTER_API_KEYç¯å¢ƒå˜é‡")
            return
    
    try:
        eval_model = create_openrouter_model(eval_model_name, api_key)
        print(f"âœ… æˆåŠŸé…ç½®è¯„ä¼°æ¨¡å‹: {eval_model_name}")
    except Exception as e:
        print(f"âŒ é…ç½®è¯„ä¼°æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # 8. ç”Ÿæˆæˆ–åŠ è½½è¯„ä¼°æ ‡å‡†
    evaluation_steps_file = "output/evaluation_steps.json"
    
    if os.path.exists(evaluation_steps_file) and not args.regen:
        print(f"\nå‘ç°å·²å­˜åœ¨çš„è¯„ä¼°æ ‡å‡†æ–‡ä»¶: {evaluation_steps_file}")
        use_existing = input("æ˜¯å¦ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°æ ‡å‡†ï¼Ÿ(y/n): ").strip().lower()
        
        if use_existing == 'y':
            evaluation_steps = load_evaluation_steps(evaluation_steps_file)
            if evaluation_steps:
                print("å·²åŠ è½½ç°æœ‰è¯„ä¼°æ ‡å‡†")
            else:
                print("åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ")
                evaluation_steps = None
        else:
            eval_requirement = input("è¯·è¾“å…¥æ–°çš„è¯„ä¼°éœ€æ±‚: ")
            evaluation_steps = None
    else:
        eval_requirement = input("è¯·è¾“å…¥æ–°çš„è¯„ä¼°éœ€æ±‚: ")
        evaluation_steps = None
    
    # å¦‚æœæ²¡æœ‰è¯„ä¼°æ ‡å‡†ï¼Œåˆ™ç”Ÿæˆæ–°çš„
    if not evaluation_steps:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬ä½œä¸ºæ ·æœ¬æ¥ç”Ÿæˆè¯„ä¼°æ ‡å‡†
        sample_filename = list(text_files.keys())[0]
        sample_text = text_files[sample_filename]
        
        print(f"\nä½¿ç”¨æ–‡ä»¶ '{sample_filename}' ä½œä¸ºæ ·æœ¬ç”Ÿæˆè¯„ä¼°æ ‡å‡†...")
        try:
            print(f"ä½¿ç”¨ {eval_model.get_model_name()} åšè¯„ä¼°æ ‡å‡†çš„ç”Ÿæˆ")
            evaluation_steps = generate_evaluation_standard(sample_text, eval_requirement, eval_model)
            # ä¿å­˜è¯„ä¼°æ ‡å‡†
            save_evaluation_steps(evaluation_steps, evaluation_steps_file)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¯„ä¼°æ ‡å‡†å¤±è´¥: {e}")
            return
    
    print(f"\nå½“å‰è¯„ä¼°æ ‡å‡†åŒ…å« {len(evaluation_steps)} ä¸ªæ­¥éª¤:")
    for i, step in enumerate(evaluation_steps, 1):
        print(f"  {i}. {step}")
    print()
    
    # 9. è¯„ä¼°æ‰€æœ‰æ–‡æœ¬
    try:
        eval_results = evaluate_texts(text_files, user_requirement, evaluation_steps, eval_model)
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}")
        return
    
    # 10. æ’åºå¹¶æ˜¾ç¤ºè¯„ä¼°ç»“æœ
    eval_results.sort(key=lambda x: x[1], reverse=True)  # æŒ‰åˆ†æ•°é™åºæ’åˆ—
    
    print("\n=== è¯„ä¼°ç»“æœ ===")
    print(f"{'æ’å':<4} {'æ–‡ä»¶å':<30} {'åˆ†æ•°':<8} {'è¯„ä¼°ç†ç”±'}")
    print("-" * 100)
    
    for rank, (filename, score, reason) in enumerate(eval_results, 1):
        print(f"{rank:<4} {filename:<30} {score:.3f}    {reason}")
    
    # 11. æ˜¾ç¤ºæœ€ä½³ç»“æœ
    best_file, best_score, best_reason = eval_results[0]
    print(f"\nğŸ† å¾—åˆ†æœ€é«˜çš„æ–‡æœ¬:")
    print(f"æ–‡ä»¶å: {best_file}")
    print(f"åˆ†æ•°: {best_score:.3f}")
    print(f"è¯„ä¼°ç†ç”±: {best_reason}")
    
    # ä»æ–‡ä»¶åä¸­æå–æ¨¡å‹åç§°ï¼ˆå¦‚æœæ˜¯ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
    best_model = "æœªçŸ¥"
    if generation_results:
        for result in generation_results:
            if result["filename"] == best_file:
                best_model = result["model"]
                break
    else:
        # å°è¯•ä»æ–‡ä»¶åä¸­æå–æ¨¡å‹ä¿¡æ¯
        if "_" in best_file:
            best_model = best_file.split("_")[0].replace("_", "/")
    
    print(f"ç”Ÿæˆæ¨¡å‹: {best_model}")
    print(f"è¯„ä¼°æ¨¡å‹: {eval_model_name}")
    
    # 12. ä¿å­˜è¯¦ç»†ç»“æœ
    # ç¡®ä¿outputæ–‡ä»¶å¤¹å­˜åœ¨
    output_dir = "./output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    results_file = f"output/evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    detailed_results = {
        "generation_time": datetime.now().isoformat(),
        "mode": "regenerate" if args.regen else "evaluate_existing",
        "subject": args.subject,
        "user_requirement": user_requirement,
        "prompt_file": "prompts/1.prompts" if args.regen else None,
        "generation_models": model_list if args.regen else [],
        "evaluation_model": eval_model_name,
        "evaluation_steps": evaluation_steps,
        "generation_results": generation_results if args.regen else [],
        "evaluation_results": [
            {
                "rank": rank,
                "filename": filename,
                "score": score,
                "reason": reason,
                "content": text_files[filename]
            }
            for rank, (filename, score, reason) in enumerate(eval_results, 1)
        ],
        "best_result": {
            "filename": best_file,
            "score": best_score,
            "reason": best_reason,
            "model": best_model
        }
    }
    
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, indent=4, ensure_ascii=False)
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ä¿å­˜åœ¨: ./input/ ç›®å½•")
    print(f"ğŸ“Š è¯„ä¼°ç»“æœä¿å­˜åœ¨: ./output/ ç›®å½•")
    
    # 13. æ˜¾ç¤ºæœ€ä½³å†…å®¹é¢„è§ˆ
    print(f"\nğŸ“– æœ€ä½³å†…å®¹é¢„è§ˆ ({best_file}):")
    print("-" * 50)
    best_content = text_files[best_file]
    preview_length = 500
    if len(best_content) > preview_length:
        print(best_content[:preview_length] + "...")
    else:
        print(best_content)
    print("-" * 50)
    
    # 14. æ˜¾ç¤ºä½¿ç”¨æç¤º
    if not args.regen:
        print("\nğŸ’¡ æç¤ºï¼š")
        print("  - ä½¿ç”¨ --regen å‚æ•°å¯ä»¥é‡æ–°ç”Ÿæˆå†…å®¹")
        print("  - ä½¿ç”¨ --subject \"æ–°ä¸»é¢˜\" å¯ä»¥æŒ‡å®šä¸åŒçš„ç”Ÿæˆä¸»é¢˜")
        print("  - ä½¿ç”¨ --eval-model \"æ¨¡å‹å\" å¯ä»¥æŒ‡å®šä¸åŒçš„è¯„ä¼°æ¨¡å‹")

if __name__ == "__main__":
    main()