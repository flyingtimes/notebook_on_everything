import os
import json
import glob
import argparse
from datetime import datetime
from typing import List, Dict, Tuple
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models import GPTModel
from dotenv import load_dotenv
load_dotenv()
from deepeval.models.base_model import DeepEvalBaseLLM
from openai import OpenAI
from jinja2 import Template

class OpenRouterModel(DeepEvalBaseLLM):
    def __init__(self, model: str, api_key: str):
        self.api_key = api_key
        super().__init__(model)
        # ç¡®ä¿ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹åç§°ï¼Œè¦†ç›–çˆ¶ç±»å¯èƒ½çš„ä¿®æ”¹
        self.model_name = model
      
    def load_model(self):
        return OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.api_key
        )
      
    def generate(self, prompt: str) -> str:
        client = self.load_model()
        response = client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
      
    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)
      
    def get_model_name(self):
        return self.model_name

def create_openrouter_model(model_name: str = "openai/gpt-4", api_key: str = None):
    """åˆ›å»ºOpenRouteræ¨¡å‹å®ä¾‹"""
    if not api_key:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡æˆ–æä¾›api_keyå‚æ•°")
      
    return OpenRouterModel(
        model=model_name,
        api_key=api_key
    )

def get_model_list() -> List[str]:
    """ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åˆ—è¡¨"""
    model_list_str = os.getenv("MODEL_LIST", "openai/gpt-4")
    return [model.strip() for model in model_list_str.split(",") if model.strip()]

def get_all_prompt_templates(prompts_dir: str = "prompts") -> Dict[str, str]:
    """è¯»å–promptsæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¨¡æ¿æ–‡ä»¶"""
    templates = {}
    
    if not os.path.exists(prompts_dir):
        print(f"âŒ promptsç›®å½• {prompts_dir} ä¸å­˜åœ¨")
        return templates
    
    # æŸ¥æ‰¾æ‰€æœ‰.promptsæ–‡ä»¶
    prompt_files = glob.glob(os.path.join(prompts_dir, "*.prompts"))
    
    if not prompt_files:
        print(f"âŒ åœ¨ {prompts_dir} ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•.promptsæ–‡ä»¶")
        return templates
    
    for file_path in prompt_files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
                
            # è·å–æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰ä½œä¸ºæ¨¡æ¿å
            template_name = os.path.splitext(os.path.basename(file_path))[0]
            templates[template_name] = content
            print(f"âœ… æˆåŠŸè¯»å–æ¨¡æ¿: {template_name} (é•¿åº¦: {len(content)} å­—ç¬¦)")
            
        except Exception as e:
            print(f"âŒ è¯»å–æ¨¡æ¿æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return templates

def read_prompt_template(file_path: str = "prompts/2.prompts", subject: str = None) -> str:
    """è¯»å–æç¤ºè¯æ¨¡æ¿æ–‡ä»¶å¹¶å¡«å……å˜é‡"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            template_content = f.read().strip()
            
        # ä½¿ç”¨Jinja2æ¨¡æ¿å¼•æ“å¤„ç†æ¨¡æ¿
        template = Template(template_content)
        
        # å¦‚æœæä¾›äº†subjectå‚æ•°ï¼Œåˆ™è¿›è¡Œå˜é‡æ›¿æ¢
        if subject:
            content = template.render(subject=subject)
        else:
            content = template_content
            
        return content
    except FileNotFoundError:
        print(f"æç¤ºè¯æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return None
    except Exception as e:
        print(f"è¯»å–æç¤ºè¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def sanitize_filename(name: str) -> str:
    """æ¸…ç†åç§°ï¼Œä½¿å…¶é€‚åˆä½œä¸ºæ–‡ä»¶å"""
    # æ›¿æ¢ä¸é€‚åˆæ–‡ä»¶åçš„å­—ç¬¦
    return name.replace("/", "_").replace(":", "_").replace("?", "_").replace("*", "_").replace("<", "_").replace(">", "_").replace("|", "_").replace('"', "_")

def generate_content_with_templates_and_models(templates: Dict[str, str], model_list: List[str], api_key: str, subject: str = None):
    """ä½¿ç”¨å¤šä¸ªæ¨¡æ¿å’Œå¤šä¸ªæ¨¡å‹ç”Ÿæˆå†…å®¹å¹¶ä¿å­˜åˆ°inputæ–‡ä»¶å¤¹"""
    # ç¡®ä¿inputæ–‡ä»¶å¤¹å­˜åœ¨
    input_dir = "./input"
    if not os.path.exists(input_dir):
        os.makedirs(input_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {input_dir}")
    
    all_results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    total_combinations = len(templates) * len(model_list)
    current_combination = 0
    
    print(f"\nå¼€å§‹ç”Ÿæˆå†…å®¹: {len(templates)} ä¸ªæ¨¡æ¿ Ã— {len(model_list)} ä¸ªæ¨¡å‹ = {total_combinations} ä¸ªç»„åˆ\n")
    
    for template_name, template_content in templates.items():
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨å¤„ç†æ¨¡æ¿: {template_name}")
        print(f"{'='*60}")
        
        # ä½¿ç”¨Jinja2æ¨¡æ¿å¼•æ“å¤„ç†æ¨¡æ¿
        try:
            template = Template(template_content)
            if subject:
                processed_template = template.render(subject=subject)
            else:
                processed_template = template_content
        except Exception as e:
            print(f"âŒ å¤„ç†æ¨¡æ¿ {template_name} æ—¶å‡ºé”™: {e}")
            continue
        
        template_results = []
        
        for i, model_name in enumerate(model_list, 1):
            current_combination += 1
            print(f"\n[{current_combination}/{total_combinations}] æ¨¡æ¿: {template_name} | æ¨¡å‹: {model_name}")
            
            try:
                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = create_openrouter_model(model_name, api_key)
                
                # ç”Ÿæˆå†…å®¹
                print("æ­£åœ¨ç”Ÿæˆå†…å®¹...")
                generated_content = model.generate(processed_template)
                # å»æ‰ç©ºè¡Œ
                generated_content = "\n".join(line for line in generated_content.split("\n") if line.strip())
                
                # åˆ›å»ºæ–‡ä»¶åï¼šæ¨¡æ¿å_æ¨¡å‹å_æ—¶é—´æˆ³.txt
                safe_template_name = sanitize_filename(template_name)
                safe_model_name = sanitize_filename(model_name)
                filename = f"{safe_template_name}_{safe_model_name}_{timestamp}.txt"
                file_path = os.path.join(input_dir, filename)
                
                # ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(generated_content)
                
                print(f"âœ… å†…å®¹å·²ä¿å­˜åˆ°: {file_path}")
                
                # è®°å½•ç»“æœ
                result = {
                    "template_name": template_name,
                    "model": model_name,
                    "filename": filename,
                    "file_path": file_path,
                    "content_length": len(generated_content),
                    "status": "success",
                    "content": generated_content  # æ·»åŠ å†…å®¹ç”¨äºåç»­è¯„ä¼°
                }
                template_results.append(result)
                all_results.append(result)
                
            except Exception as e:
                print(f"âŒ æ¨¡æ¿ {template_name} + æ¨¡å‹ {model_name} ç”Ÿæˆå¤±è´¥: {e}")
                result = {
                    "template_name": template_name,
                    "model": model_name,
                    "filename": None,
                    "file_path": None,
                    "content_length": 0,
                    "status": "failed",
                    "error": str(e),
                    "content": None
                }
                template_results.append(result)
                all_results.append(result)
        
        # æ˜¾ç¤ºå½“å‰æ¨¡æ¿çš„ç»“æœæ‘˜è¦
        successful_count = sum(1 for r in template_results if r["status"] == "success")
        failed_count = len(template_results) - successful_count
        print(f"\næ¨¡æ¿ '{template_name}' ç»“æœæ‘˜è¦:")
        print(f"  æˆåŠŸç”Ÿæˆ: {successful_count}/{len(model_list)}")
        print(f"  ç”Ÿæˆå¤±è´¥: {failed_count}/{len(model_list)}")
    
    return all_results

def read_text_files(directory: str = "./input") -> Dict[str, str]:
    """è¯»å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
    text_files = {}
      
    # æ”¯æŒå¤šç§æ–‡æœ¬æ–‡ä»¶æ ¼å¼
    patterns = ["*.txt", "*.md", "*.text"]
      
    for pattern in patterns:
        file_pattern = os.path.join(directory, pattern)
        for file_path in glob.glob(file_pattern):
            filename = os.path.basename(file_path)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    # æå–å®é™…ç”Ÿæˆçš„å†…å®¹ï¼ˆè·³è¿‡æ–‡ä»¶å¤´ä¿¡æ¯ï¼‰
                    if "=" * 50 in content:
                        content = content.split("=" * 50, 1)[1].strip()
                    if content:  # åªä¿å­˜éç©ºæ–‡ä»¶
                        text_files[filename] = content
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
      
    return text_files

def generate_evaluation_standard(sample_text: str, user_requirement: str, model: OpenRouterModel) -> List[str]:
    """ä½¿ç”¨G-Evalç”Ÿæˆè¯„ä¼°æ ‡å‡†"""
    print("æ­£åœ¨ç”Ÿæˆè¯„ä¼°æ ‡å‡†...")
      
    # åˆ›å»ºG-EvalæŒ‡æ ‡æ¥ç”Ÿæˆè¯„ä¼°æ­¥éª¤
    criteria = f"""
    æ ¹æ®ç”¨æˆ·éœ€æ±‚è¯„ä¼°AIç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}
      
    """
      
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æµ‹è¯•ç”¨ä¾‹æ¥ç”Ÿæˆè¯„ä¼°æ­¥éª¤
    temp_test_case = LLMTestCase(
        input=user_requirement,
        actual_output=sample_text
    )
      
    temp_metric = GEval(
        name="æ–‡æœ¬è´¨é‡è¯„ä¼°",
        criteria=criteria,
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        model=model,  # ä½¿ç”¨OpenRouteræ¨¡å‹
        verbose_mode=False
    )
      
    # è¿è¡Œä¸€æ¬¡ä»¥ç”Ÿæˆè¯„ä¼°æ­¥éª¤
    temp_metric.measure(temp_test_case)
      
    return temp_metric.evaluation_steps

def evaluate_texts(text_files: Dict[str, str], user_requirement: str,   
                  evaluation_steps: List[str], model: OpenRouterModel) -> List[Tuple[str, float, str]]:
    """ä½¿ç”¨å›ºå®šçš„è¯„ä¼°æ­¥éª¤è¯„ä¼°æ‰€æœ‰æ–‡æœ¬"""
    print("å¼€å§‹è¯„ä¼°æ–‡æœ¬...")
      
    # åˆ›å»ºå›ºå®šè¯„ä¼°æ­¥éª¤çš„G-EvalæŒ‡æ ‡
    quality_metric = GEval(
        name="æ–‡æœ¬è´¨é‡è¯„ä¼°",
        evaluation_steps=evaluation_steps,
        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        model=model,  # ä½¿ç”¨OpenRouteræ¨¡å‹
        threshold=0.7,
        verbose_mode=True 
    )
      
    results = []
      
    for filename, content in text_files.items():
        # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
        test_case = LLMTestCase(
            input=user_requirement,
            actual_output=content
        )
          
        # è¯„ä¼°
        score = quality_metric.measure(test_case)
        reason = quality_metric.reason
          
        results.append((filename, score, reason))
        print(f"è¯„ä¼°æ–‡ä»¶: {filename}, åˆ†æ•°: {score:.3f}")
      
    return results

def save_evaluation_steps(steps: List[str], file_path: str = "output/evaluation_steps.json"):
    """ä¿å­˜è¯„ä¼°æ­¥éª¤åˆ°JSONæ–‡ä»¶"""
    # ç¡®ä¿outputæ–‡ä»¶å¤¹å­˜åœ¨
    output_dir = os.path.dirname(file_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    data = {
        "evaluation_steps": steps,
        "created_at": datetime.now().isoformat(),
        "version": "1.0"
    }
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"è¯„ä¼°æ­¥éª¤å·²ä¿å­˜åˆ°: {file_path}")

def load_evaluation_steps(file_path: str = "output/evaluation_steps.json") -> List[str]:
    """ä»JSONæ–‡ä»¶åŠ è½½è¯„ä¼°æ­¥éª¤"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data["evaluation_steps"]
    except FileNotFoundError:
        print(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        return None

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="AIå¤šæ¨¡å‹å†…å®¹ç”Ÿæˆä¸è¯„ä¼°ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py                    # ç›´æ¥è¯„ä¼°inputæ–‡ä»¶å¤¹ä¸­çš„ç°æœ‰æ–‡ä»¶
  python main.py --regen            # é‡æ–°ç”Ÿæˆå†…å®¹å¹¶è¯„ä¼°
  python main.py --regen --subject "æ–°çš„ä¸»é¢˜"  # ä½¿ç”¨æ–°ä¸»é¢˜é‡æ–°ç”Ÿæˆå†…å®¹
        """
    )
    
    parser.add_argument(
        "--regen", 
        action="store_true", 
        help="é‡æ–°ç”Ÿæˆå†…å®¹ã€‚å¦‚æœä¸ä½¿ç”¨æ­¤å‚æ•°ï¼Œç¨‹åºå°†ç›´æ¥è¯„ä¼°inputæ–‡ä»¶å¤¹ä¸­çš„ç°æœ‰æ–‡ä»¶"
    )
    
    parser.add_argument(
        "--subject", 
        type=str, 
        default="å¦‚ä½•æå‡äºŒå¹´çº§å°å­¦ç”Ÿçš„å†…é©±åŠ›",
        help="ç”Ÿæˆå†…å®¹çš„ä¸»é¢˜ï¼ˆé»˜è®¤ï¼šå¦‚ä½•æå‡äºŒå¹´çº§å°å­¦ç”Ÿçš„å†…é©±åŠ›ï¼‰"
    )
    
    parser.add_argument(
        "--eval-model", 
        type=str, 
        help="æŒ‡å®šè¯„ä¼°æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡EVAL_MODELæˆ–ç¬¬ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼‰"
    )
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    print("=== AIå¤šæ¨¡å‹å†…å®¹ç”Ÿæˆä¸è¯„ä¼°ç³»ç»Ÿ ===")
    print("æ”¯æŒå¤šæ¨¡æ¿å¤„ç†\n")
    
    if args.regen:
        print("ğŸ”„ æ¨¡å¼ï¼šé‡æ–°ç”Ÿæˆå†…å®¹å¹¶è¯„ä¼°")
    else:
        print("ğŸ“ æ¨¡å¼ï¼šç›´æ¥è¯„ä¼°ç°æœ‰æ–‡ä»¶")
    
    print(f"ğŸ“ ä¸»é¢˜ï¼š{args.subject}\n")
    
    # å®šä¹‰ç”¨æˆ·éœ€æ±‚ï¼ˆåŸºäºä¸»é¢˜ï¼‰
    user_requirement = f"{args.subject}"
    
    # 1. æ£€æŸ¥APIå¯†é’¥
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENROUTER_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # 2. è·å–æ¨¡å‹åˆ—è¡¨å’Œæ¨¡æ¿ï¼ˆä»…åœ¨éœ€è¦ç”Ÿæˆå†…å®¹æ—¶ï¼‰
    model_list = []
    generation_results = []
    
    if args.regen:
        model_list = get_model_list()
        print(f"ä»ç¯å¢ƒå˜é‡è¯»å–åˆ° {len(model_list)} ä¸ªæ¨¡å‹:")
        for i, model in enumerate(model_list, 1):
            print(f"  {i}. {model}")
        print()
        
        # 3. è¯»å–æ‰€æœ‰æç¤ºè¯æ¨¡æ¿
        templates = get_all_prompt_templates("prompts")
        if not templates:
            print("âŒ æ— æ³•è¯»å–ä»»ä½•æç¤ºè¯æ¨¡æ¿ï¼Œç¨‹åºé€€å‡º")
            return
        
        print(f"\næˆåŠŸè¯»å– {len(templates)} ä¸ªæ¨¡æ¿:")
        for template_name in templates.keys():
            print(f"  - {template_name}")
        print()
        
        # 4. ç”Ÿæˆå†…å®¹
        print("å¼€å§‹ä½¿ç”¨å¤šä¸ªæ¨¡æ¿å’Œå¤šä¸ªæ¨¡å‹ç”Ÿæˆå†…å®¹...")
        generation_results = generate_content_with_templates_and_models(templates, model_list, api_key, args.subject)
        
        # 5. æ˜¾ç¤ºç”Ÿæˆç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("=== æ€»ä½“ç”Ÿæˆç»“æœæ‘˜è¦ ===")
        print("=" * 60)
        
        successful_count = sum(1 for r in generation_results if r["status"] == "success")
        failed_count = len(generation_results) - successful_count
        
        print(f"æ€»ç»„åˆæ•°: {len(generation_results)}")
        print(f"æˆåŠŸç”Ÿæˆ: {successful_count}")
        print(f"ç”Ÿæˆå¤±è´¥: {failed_count}")
        
        # æŒ‰æ¨¡æ¿åˆ†ç»„æ˜¾ç¤ºç»“æœ
        template_summary = {}
        for result in generation_results:
            template_name = result["template_name"]
            if template_name not in template_summary:
                template_summary[template_name] = {"success": 0, "failed": 0}
            
            if result["status"] == "success":
                template_summary[template_name]["success"] += 1
            else:
                template_summary[template_name]["failed"] += 1
        
        print("\nå„æ¨¡æ¿ç”Ÿæˆæƒ…å†µ:")
        for template_name, counts in template_summary.items():
            total = counts["success"] + counts["failed"]
            print(f"  {template_name}: {counts['success']}/{total} æˆåŠŸ")
        
        if successful_count == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•å†…å®¹ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return
    else:
        print("â­ï¸  è·³è¿‡å†…å®¹ç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶")
    
    # 6. å¼€å§‹è¯„ä¼°é˜¶æ®µ
    print("\n" + "=" * 50)
    print("å¼€å§‹è¯„ä¼°é˜¶æ®µ...")
    print("=" * 50)
    
    # æ£€æŸ¥inputæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨æ–‡ä»¶
    if not os.path.exists("./input"):
        print("âŒ inputæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        if not args.regen:
            print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --regen å‚æ•°å¯ä»¥é‡æ–°ç”Ÿæˆå†…å®¹")
        return
    
    # 7. è¯»å–ç”Ÿæˆçš„æ–‡æœ¬æ–‡ä»¶
    text_files = read_text_files("./input")
    if not text_files:
        print("âŒ åœ¨ ./input ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶")
        if not args.regen:
            print("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --regen å‚æ•°å¯ä»¥é‡æ–°ç”Ÿæˆå†…å®¹")
        return
    
    print(f"\næ‰¾åˆ° {len(text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶è¿›è¡Œè¯„ä¼°:")
    for filename in text_files.keys():
        print(f"  - {filename}")
    print()
    
    # é…ç½®è¯„ä¼°æ¨¡å‹
    eval_model_name = args.eval_model or os.getenv("OPENROUTER_MODEL")
    if not eval_model_name:
        if model_list:
            eval_model_name = model_list[0]
        else:
            eval_model_name = "openai/gpt-4"  # é»˜è®¤è¯„ä¼°æ¨¡å‹
    
    print(f"ä½¿ç”¨è¯„ä¼°æ¨¡å‹: {eval_model_name}")
    
    try:
        eval_model = create_openrouter_model(eval_model_name, api_key)
        print(f"âœ… æˆåŠŸé…ç½®è¯„ä¼°æ¨¡å‹: {eval_model_name}")
    except Exception as e:
        print(f"âŒ é…ç½®è¯„ä¼°æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # 8. ç”Ÿæˆæˆ–åŠ è½½è¯„ä¼°æ ‡å‡†
    evaluation_steps_file = "output/evaluation_steps.json"
    eval_requirement = user_requirement  # é»˜è®¤ä½¿ç”¨ç”¨æˆ·éœ€æ±‚ä½œä¸ºè¯„ä¼°éœ€æ±‚
    
    if os.path.exists(evaluation_steps_file) and not args.regen:
        print(f"\nå‘ç°å·²å­˜åœ¨çš„è¯„ä¼°æ ‡å‡†æ–‡ä»¶: {evaluation_steps_file}")
        use_existing = input("æ˜¯å¦ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°æ ‡å‡†ï¼Ÿ(y/n): ").strip().lower()
        
        if use_existing == 'y':
            evaluation_steps = load_evaluation_steps(evaluation_steps_file)
            if evaluation_steps:
                print("å·²åŠ è½½ç°æœ‰è¯„ä¼°æ ‡å‡†")
                # ä½¿ç”¨é»˜è®¤çš„ç”¨æˆ·éœ€æ±‚ä½œä¸ºè¯„ä¼°éœ€æ±‚
                print(f"ä½¿ç”¨è¯„ä¼°éœ€æ±‚: {eval_requirement}")
            else:
                print("åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ")
                eval_requirement = input("è¯·è¾“å…¥è¯„ä¼°éœ€æ±‚: ")
                evaluation_steps = None
        else:
            eval_requirement = input("è¯·è¾“å…¥æ–°çš„è¯„ä¼°éœ€æ±‚: ")
            evaluation_steps = None
    else:
        eval_requirement = input("è¯·è¾“å…¥è¯„ä¼°éœ€æ±‚: ")
        evaluation_steps = None
    
    # å¦‚æœæ²¡æœ‰è¯„ä¼°æ ‡å‡†ï¼Œåˆ™ç”Ÿæˆæ–°çš„
    if not evaluation_steps:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬ä½œä¸ºæ ·æœ¬æ¥ç”Ÿæˆè¯„ä¼°æ ‡å‡†
        sample_filename = list(text_files.keys())[0]
        sample_text = text_files[sample_filename]
        
        print(f"\nä½¿ç”¨æ–‡ä»¶ '{sample_filename}' ä½œä¸ºæ ·æœ¬ç”Ÿæˆè¯„ä¼°æ ‡å‡†...")
        try:
            print(f"ä½¿ç”¨ {eval_model.get_model_name()} åšè¯„ä¼°æ ‡å‡†çš„ç”Ÿæˆ")
            evaluation_steps = generate_evaluation_standard(sample_text, eval_requirement, eval_model)
            # ä¿å­˜è¯„ä¼°æ ‡å‡†
            save_evaluation_steps(evaluation_steps, evaluation_steps_file)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¯„ä¼°æ ‡å‡†å¤±è´¥: {e}")
            return
    
    print(f"\nå½“å‰è¯„ä¼°æ ‡å‡†åŒ…å« {len(evaluation_steps)} ä¸ªæ­¥éª¤:")
    for i, step in enumerate(evaluation_steps, 1):
        print(f"  {i}. {step}")
    print()
    
    # 9. è¯„ä¼°æ‰€æœ‰æ–‡æœ¬
    try:
        eval_results = evaluate_texts(text_files, eval_requirement, evaluation_steps, eval_model)
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}")
        return
    
    # 10. æŒ‰åˆ†æ•°æ’åºå¹¶æ˜¾ç¤ºç»“æœ
    eval_results.sort(key=lambda x: x[1], reverse=True)
    
    print("\n" + "=" * 50)
    print("=== è¯„ä¼°ç»“æœ (æŒ‰åˆ†æ•°æ’åº) ===")
    print("=" * 50)
    
    for i, (filename, score, reason) in enumerate(eval_results, 1):
        print(f"\n{i}. æ–‡ä»¶: {filename}")
        print(f"   åˆ†æ•°: {score:.3f}")
        print(f"   è¯„ä»·: {reason[:200]}{'...' if len(reason) > 200 else ''}")
    
    # 11. çªå‡ºæ˜¾ç¤ºæœ€ä½³ç»“æœ
    if eval_results:
        best_filename, best_score, best_reason = eval_results[0]
        print("\n" + "=" * 50)
        print("ğŸ† æœ€ä½³ç»“æœ")
        print("=" * 50)
        print(f"æ–‡ä»¶å: {best_filename}")
        print(f"åˆ†æ•°: {best_score:.3f}")
        print(f"è¯„ä»·: {best_reason}")
        
        # æ˜¾ç¤ºæœ€ä½³å†…å®¹çš„é¢„è§ˆ
        best_content = text_files[best_filename]
        print(f"\nå†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):")
        print("-" * 30)
        print(best_content[:500])
        if len(best_content) > 500:
            print("...")
        print("-" * 30)
    
    # 12. ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"output/evaluation_results_{timestamp}.json"
    
    # ç¡®ä¿outputæ–‡ä»¶å¤¹å­˜åœ¨
    output_dir = os.path.dirname(results_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    detailed_results = {
        "evaluation_time": datetime.now().isoformat(),
        "user_requirement": eval_requirement,
        "eval_model": eval_model_name,
        "total_files": len(text_files),
        "evaluation_steps": evaluation_steps,
        "results": [
            {
                "rank": i + 1,
                "filename": filename,
                "score": score,
                "reason": reason,
                "content": text_files[filename]
            }
            for i, (filename, score, reason) in enumerate(eval_results)
        ]
    }
    
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, indent=4, ensure_ascii=False)
    
    print(f"\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print("\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    main()