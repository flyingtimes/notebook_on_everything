{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "%conda create -n distilabel python=3.12.9\n",
    "%conda activate distilabel\n",
    "\n",
    "## 安装基础sdk包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install distilabel[ollama,openai]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先测试一下普通llm的应答效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13802\\AppData\\Local\\Temp\\ipykernel_23860\\786400136.py:4: UserWarning: A custom validator is returning a value other than `self`.\n",
      "Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\n",
      "See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\n",
      "  llm = OllamaLLM(model=\"llama3.2-vision:latest\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，世界！\n"
     ]
    }
   ],
   "source": [
    "from distilabel.models.llms import OllamaLLM\n",
    "from distilabel.models.llms import OpenAILLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2-vision:latest\")\n",
    "#llm = OllamaLLM(model=\"deepseek-r1:7b\")\n",
    "#llm = OpenAILLM(model=\"deepseek-r1:7b\",base_url=\"http://127.0.0.1:11434/v1\",api_key=\"akey\")\n",
    "llm.load()\n",
    "\n",
    "# Call the model\n",
    "output = llm.generate(inputs=[[{\"role\": \"system\", \"content\": \"You are a helpful assistant,you always response in chinese\"},{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n",
    "print(output[0]['generations'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepseek 的响应\n",
    "7b的响应不正常，14b的响应相对ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.models.llms import OllamaLLM\n",
    "from distilabel.models.llms import OpenAILLM\n",
    "\n",
    "#llm = OllamaLLM(model=\"llama3.2-vision:latest\")\n",
    "llm = OllamaLLM(model=\"deepseek-r1:14b\")\n",
    "#llm = OpenAILLM(model=\"deepseek-r1:7b\",base_url=\"http://127.0.0.1:11434/v1\",api_key=\"akey\")\n",
    "llm.load()\n",
    "# Call the model\n",
    "output = llm.generate(inputs=[[{\"role\": \"system\", \"content\": \"You are a helpful assistant,you always response in chinese\"},{\"role\": \"user\", \"content\": \"Hello world!\"}]])\n",
    "print(output)\n",
    "\n",
    "extracted_content = output[0]['generations'][0].split('</think>\\n\\n')[1]\n",
    "print(extracted_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.steps import LoadDataFromHub\n",
    "import os\n",
    "os.environ[\"ALL_PROXY\"] = \"http://127.0.0.1:1087\"\n",
    "loader = LoadDataFromHub(\n",
    "    repo_id=\"Conard/fortune-telling\",\n",
    "    split=\"train\")\n",
    "loader.load()\n",
    "(result,_)=next(loader.process())\n",
    "for item in result:\n",
    "    print(item['Question'])\n",
    "    print(item['Response'])\n",
    "    print(item['Complex_CoT'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比大模型和语料输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 'None' hasn't received a pipeline, and it hasn't been created within a `Pipeline` context. Please, use `with Pipeline() as pipeline:` and create the step within the context.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "问题:新房装修,大门对着电梯好不好?要如何化解?\n",
      "----------------------------------------------------------\n",
      "数据集的应答:根据传统风水学的观点，大门正对电梯易形成\"开口煞\"，电梯频繁升降会扰乱家宅气场。建议化解方案：\n",
      "\n",
      "1. 玄关阻隔法\n",
      "在入门处设置L型屏风或文化砖玄关墙，高度以1.8米为宜，既保持采光又形成缓冲带\n",
      "\n",
      "2. 五行通关法\n",
      "门槛石下埋设五帝钱+白玉葫芦，建议选丙申年铸造的真品古币，配合门楣悬挂九宫八卦镜\n",
      "\n",
      "3. 光影化解术\n",
      "安装磨砂玻璃内推门，门框镶嵌黄铜门槛，每日辰时用海盐净化门廊区域\n",
      "\n",
      "4. 现代科技方案\n",
      "入户区安装智能感应灯带，设置循环播放的流水声效，运用声光电技术平衡磁场\n",
      "\n",
      "需注意电梯井方位与家主命卦的关系，建议提供具体户型平面图进行吉凶方位测算。当代建筑中可采用半透明艺术隔断结合空气净化系统，既符合科学原理又兼顾传统智慧。\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AsyncLLM.__del__ at 0x0000015900BFB6A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\13802\\miniconda3\\envs\\distilabel\\Lib\\site-packages\\distilabel\\models\\llms\\base.py\", line 421, in __del__\n",
      "    if self._new_event_loop:\n",
      "       ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\13802\\miniconda3\\envs\\distilabel\\Lib\\site-packages\\pydantic\\main.py\", line 870, in __getattr__\n",
      "    return self.__pydantic_private__[item]  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\13802\\miniconda3\\envs\\distilabel\\Lib\\site-packages\\pydantic\\main.py\", line 888, in __getattr__\n",
      "    return super().__getattribute__(item)  # Raises AttributeError if appropriate\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'OllamaLLM' object has no attribute '__pydantic_private__'\n"
     ]
    }
   ],
   "source": [
    "from distilabel.steps import LoadDataFromHub\n",
    "import os\n",
    "from distilabel.models.llms import OllamaLLM\n",
    "from distilabel.models.llms import OpenAILLM\n",
    "import warnings \n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"ALL_PROXY\"] = \"http://127.0.0.1:1087\"\n",
    "loader = LoadDataFromHub(\n",
    "    repo_id=\"Conard/fortune-telling\",\n",
    "    split=\"train\")\n",
    "loader.load()\n",
    "os.environ['ALL_PROXY'] = ''\n",
    "(result,_)=next(loader.process())\n",
    "for item in result:\n",
    "    print('----------------------------------------------------------')\n",
    "    print(f'问题:{item[\"Question\"]}')\n",
    "    print('----------------------------------------------------------')\n",
    "    print(f'数据集的应答:{item[\"Response\"]}')\n",
    "    print('----------------------------------------------------------')\n",
    "    llm = OllamaLLM(model=\"deepseek-r1:14b\")\n",
    "    llm.load()\n",
    "    output = llm.generate(inputs=[[{\"role\": \"system\", \"content\": \"You are a helpful assistant,you always response in chinese\"},{\"role\": \"user\", \"content\": item[\"Question\"]}]])\n",
    "    cot = output[0]['generations'][0].split('</think>\\n\\n')[0]\n",
    "    extracted_content = output[0]['generations'][0].split('</think>\\n\\n')[1]\n",
    "    print('----------------------------------------------------------')\n",
    "\n",
    "    print(f'ds大模型的应答:{extracted_content}')\n",
    "    output = llm.generate(inputs=[[{\"role\": \"system\", \"content\": \"You are a helpful assistant,you always response in chinese\"},{\"role\": \"user\", \"content\": item[\"Question\"]+cot}]])\n",
    "    #extracted_content = output[0]['generations'][0].split('</think>\\n\\n')[1]\n",
    "    print('----------------------------------------------------------')\n",
    "    print(f'ds大模型使用cot的应答:{output[0]['generations'][0]}')\n",
    "    #print(f'数据集的思维链:{item[\"Complex_CoT\"]}')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
